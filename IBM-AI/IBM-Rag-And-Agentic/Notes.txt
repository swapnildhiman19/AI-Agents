1. GANs ( Generative Adversarial Networks ), Variational Autoencoders ( VAEs ), Transformers, Diffusion Models

Langchain: Retrieval, Extraction, Procsessing, Generation 

LCEL

# Part 2: LCEL (LangChain Expression Language) -- The Big Picture

Now let's tackle those LangChain concepts. I'll go one by one.

## The Real-World Analogy: A Factory Assembly Line

```
LCEL = A system for connecting workers on an assembly line

Imagine a SANDWICH SHOP:

  [Take Order] --> [Get Bread] --> [Add Filling] --> [Wrap It] --> [Serve]

Each worker does ONE thing and passes the result to the next.
That's what LCEL does with AI components.
```

## Concept 1: Chain / Pipe (`|`) Operator

Your current code already shows the simplest LangChain usage:

```1:7:/Users/s0d0bla/Desktop/AI/AI-Agents/IBM-AI/IBM-Rag-And-Agentic/langchain_intro.py
from langchain_core.prompts import PromptTemplate
prompt_template = PromptTemplate.from_template(
    "Tell me a {adjective} joke about {content}"
)

prompt = prompt_template.format(adjective="funny", content="cats")
print(prompt)
```

But this only does ONE step. LCEL lets you **chain multiple steps** together using the pipe `|` operator:

```python
# WITHOUT LCEL (manual, step by step):
prompt = prompt_template.format(adjective="funny", content="cats")
response = llm.invoke(prompt)
parsed = parser.parse(response)

# WITH LCEL (connected assembly line using | pipe):
chain = prompt_template | llm | parser
result = chain.invoke({"adjective": "funny", "content": "cats"})
```

```
VISUAL: The Pipe Operator

  prompt_template  ──|──>  llm  ──|──>  parser
       ↑                    ↑              ↑
  "Format the          "Send to        "Clean up
   question"            the AI"         the answer"

The | symbol means: "take the output of the left side
                     and feed it as input to the right side"

It's identical to the Unix pipe:  ls | grep ".py" | sort
```

## Concept 2: Runnable

**Every component in LCEL is a "Runnable."** A Runnable is just anything that has an `.invoke()` method -- meaning you can give it an input and get an output.

```
WHAT IS A RUNNABLE?

  Think of a Runnable as a LEGO brick.
  Every LEGO brick has the same connector shape (studs on top, holes on bottom).
  So any brick can connect to any other brick.

  Similarly, every Runnable has:
    .invoke(input) --> output
    .stream(input) --> output chunks
    .batch([inputs]) --> [outputs]

  This means ANY Runnable can connect to ANY other Runnable with |
```

The things that are Runnables:
- `PromptTemplate` -- a Runnable
- `ChatModel (LLM)` -- a Runnable
- `OutputParser` -- a Runnable
- Any Python function wrapped in `RunnableLambda` -- a Runnable

## Concept 3: RunnableSequence

When you use `|`, you're secretly creating a **RunnableSequence**. These two are identical:

```python
# Using pipe (shorthand):
chain = prompt | llm | parser

# Using RunnableSequence (explicit, verbose):
from langchain_core.runnables import RunnableSequence
chain = RunnableSequence(first=prompt, middle=[llm], last=parser)

# They do the EXACT same thing. The | is just prettier syntax.
```

```
RunnableSequence = A chain of steps that run ONE AFTER ANOTHER

  Step 1 ──> Step 2 ──> Step 3 ──> Final Result
  
  Each step's OUTPUT becomes the next step's INPUT.
  Like dominoes falling in order.
```

## Concept 4: RunnableParallel (Same Input, Multiple Paths)

Sometimes you want to send the **same input** to **multiple workers at the same time**:

```python
from langchain_core.runnables import RunnableParallel

# Both branches receive the SAME input simultaneously
chain = RunnableParallel(
    joke=prompt_joke | llm,      # Branch 1: generate a joke
    poem=prompt_poem | llm       # Branch 2: generate a poem
)

result = chain.invoke({"topic": "cats"})
# result = {"joke": "Why did the cat...", "poem": "Roses are red..."}
```

```
RunnableParallel = A FORK in the assembly line

                    ┌──> [Make Joke] ──> joke result ──┐
  {"topic":"cats"} ─┤                                   ├──> {"joke": ..., "poem": ...}
                    └──> [Make Poem] ──> poem result ──┘

  Same input goes to ALL branches.
  Results are collected into a dictionary.
  Branches run AT THE SAME TIME (parallel), not one after another.
```

## Concept 5: RunnableLambda (Wrap Any Function)

What if you want to use your own custom Python function in the chain? Wrap it in `RunnableLambda`:

```python
from langchain_core.runnables import RunnableLambda

# A plain Python function
def shout(text):
    return text.upper() + "!!!"

# Wrap it so it becomes a Runnable (has .invoke(), can use |)
shout_runnable = RunnableLambda(shout)

chain = prompt | llm | parser | shout_runnable
# Now the final output will be UPPERCASED!!!
```

```
RunnableLambda = An adapter plug

  Your function:     shout("hello") -> "HELLO!!!"
  As a Runnable:     shout_runnable.invoke("hello") -> "HELLO!!!"

  It's like putting a US-to-EU adapter on a plug.
  The function stays the same, but now it fits the LCEL socket.
```

## Concept 6: Type Coercion (Automatic Conversion)

LCEL is smart enough to **automatically convert** plain functions and dictionaries into Runnables, so you don't always need to manually wrap them:

```python
# You CAN write this (explicit):
chain = prompt | llm | RunnableLambda(lambda x: x.upper())

# But LCEL auto-converts, so this ALSO works (implicit):
chain = prompt | llm | (lambda x: x.upper())

# Dictionaries automatically become RunnableParallel:
chain = {"joke": joke_chain, "poem": poem_chain} | combine_step
# The dict is auto-converted to RunnableParallel(joke=..., poem=...)
```

```
TYPE COERCION = LCEL's auto-pilot

  You give it:                  LCEL sees it as:
  ─────────────                 ────────────────
  A plain function       -->    RunnableLambda(function)
  A dictionary           -->    RunnableParallel(dict)
  
  It's like your phone auto-correcting "teh" to "the"
  -- it just figures out what you meant.
```

## Concept 7: Streaming & Async

Because every component is a Runnable, you get **streaming** and **async** for free:

```python
# Normal call (wait for full response):
result = chain.invoke({"topic": "cats"})

# Streaming (get chunks as they're generated -- like ChatGPT typing):
for chunk in chain.stream({"topic": "cats"}):
    print(chunk, end="")  # prints piece by piece

# Async (non-blocking, good for web servers):
result = await chain.ainvoke({"topic": "cats"})
```

```
STREAMING = Getting the response word by word

  invoke():    .............[complete answer appears all at once]
  stream():    H.e.l.l.o., .h.e.r.e. .i.s. .y.o.u.r. .a.n.s.w.e.r...

  Like watching a live broadcast vs. waiting for the recording.
```

## Concept 8: LCEL vs. LangGraph

This is the final and most important distinction:

```
WHEN TO USE WHAT:

  LCEL (LangChain Expression Language):
  ──────────────────────────────────────
  - Simple, LINEAR flows: A -> B -> C -> Done
  - Or simple parallel: A -> {B, C} -> D -> Done
  - No decisions, no loops, no going back
  - Think: Assembly line (one direction only)

  LangGraph:
  ──────────
  - COMPLEX flows with DECISIONS and LOOPS
  - "If the answer is bad, go back and retry"
  - "If the user says X, go to step 3; if Y, go to step 7"
  - Think: A choose-your-own-adventure book

  ANALOGY:
  ┌─────────────────────────────────────────────────┐
  │ LCEL = A highway (one direction, fast, simple)  │
  │                                                  │
  │ LangGraph = A city road network (turns, loops,  │
  │             traffic lights, decisions at every   │
  │             intersection)                        │
  └─────────────────────────────────────────────────┘
```

```
LCEL FLOW:
  Start ──> Step1 ──> Step2 ──> Step3 ──> End
  (No going back, no decisions)

LANGGRAPH FLOW:
  Start ──> Step1 ──> Decision?
                        ├── Yes ──> Step2 ──> Check ──> Good? ──> End
                        │                                  │
                        │                                  └── No ──> Step1 (retry!)
                        └── No ──> Step3 ──> End
```

## Complete Summary Map

```
EVERYTHING CONNECTS LIKE THIS:

  ┌─────────────────────────────────────────────────────────────┐
  │                     LCEL ECOSYSTEM                          │
  │                                                             │
  │  RUNNABLE (base building block -- has .invoke/.stream)      │
  │    │                                                        │
  │    ├── RunnableSequence    (A | B | C)     -- steps in order│
  │    ├── RunnableParallel    ({a: X, b: Y})  -- fork & merge  │
  │    ├── RunnableLambda      (custom func)   -- your code     │
  │    └── Built-in Runnables  (Prompt, LLM, Parser, etc.)     │
  │                                                             │
  │  PIPE OPERATOR (|)                                          │
  │    = Shorthand for creating RunnableSequence                │
  │                                                             │
  │  TYPE COERCION                                              │
  │    = Auto-converts dicts -> RunnableParallel                │
  │    = Auto-converts functions -> RunnableLambda              │
  │                                                             │
  │  FREE FEATURES (because everything is a Runnable):          │
  │    - .stream()    (token-by-token output)                   │
  │    - .ainvoke()   (async support)                           │
  │    - .batch()     (process multiple inputs)                 │
  │                                                             │
  │  LIMITS: Only linear/parallel flows. No loops/decisions.    │
  │          For those ──> use LangGraph                        │
  └─────────────────────────────────────────────────────────────┘
```

---

**In short:**
- **Your GAN code** = two neural networks (Forger vs Detective) competing. The Forger learns to create realistic fake images. The Detective learns to catch fakes. They push each other to improve.
- **LCEL** = a clean way to plug AI components together using `|`, like connecting LEGO bricks on an assembly line. When you need something more complex with loops and decisions, that's where LangGraph steps in.

----------------------------------------------------------------
LangChain Core concepts:
Documents, Chains, Agents, Language Model, Chat Model, Chat Message, Prompt template, OutputParser

Language Model : Foundation LLMs
(IBM, Google, OpenAI, Meta)

model = ModelInference(model_id, parameters:, credentials, project_id)
model.generate('')

ChatModel:
model = ModelInference()
chatConversationModel = WatsonxLLM(model=model)
chatConversationModel.invoke("")

ChatMessages:
HumanMessage: Helps user inputs
AIMessage: Generated by model
SystemMessage: Helps to instruct the model { eg. SystemMessage(content="You are nice AI bot that helps a user Z to figure out , what to eat in one short sentence?")}
FunctionMessage: Helps function to call outcomes
ToolMessage: Helps in tool interaction


msg = mixtral_llm.invoke(
  [
    SystemMessage(content="You are supportive AI bot that suggests fitness activities to a user in one short sentence")
    HumanMessage(content="I like high-intensity workouts, what should I do ?")
    AIMessage(content="You should try a CrossFit class.")
    HumanMessage(content="How often should I attend")
  ]
)

The above thing contains the conversation history to our chatConversationModel if you want.
Prompt templates:
Semantic Similarly
Max marginal relevance for diversity.
Example of efficient prompts
N gram overalp for textual similarity

OutputParser:
JSON, XML, CSV, PandaDataFrames
CommaSeparatedListOutputParser()


----------------------------------------------------

13/02/2026

Chains in LangChain

Chains are sequences of calls where the output of one step serves as the input for the next, creating a seamless flow of information.
An example involves three chains: selecting a dish based on location, providing the recipe, and estimating cooking time.
Memory Storage in LangChain

Memory storage is crucial for reading and writing historical data, enhancing user inputs and preserving context across interactions.
The ChatMessageHistory class manages conversation histories, allowing for effective storage of both AI and user messages.
Agents in LangChain

Agents are dynamic systems that use a language model to determine and sequence actions, integrating with external tools to fulfill user requests.
An example includes a Pandas DataFrame agent that allows users to query and visualize data using natural language, transforming queries into executable Python code.

I can see you're at Cell 5, ready for the next step. Here's the answer:

## The Gemini equivalent of `WatsonxLLM` + `.invoke()`

In the IBM world:

```python
# IBM: Wrap the raw model in a LangChain-compatible wrapper, then use .invoke()
llama_llm = WatsonxLLM(model=model)
print(llama_llm.invoke("Who is man's best friend?"))
```

The Gemini equivalent is the `llm` you already created in your Cell 3. You can directly write:

```python
# Google: llm is already the LangChain-compatible wrapper (ChatGoogleGenerativeAI)
print(llm.invoke("Who is man's best friend?").content)
```

### Why `.content` at the end?

Here's the key difference:

```
IBM WatsonxLLM.invoke()   --> returns a plain STRING
                               "Dogs are man's best friend..."

Google ChatGoogleGenerativeAI.invoke() --> returns an AIMessage OBJECT
                                            AIMessage(content="Dogs are man's best friend...",
                                                      response_metadata={...})
```

`ChatGoogleGenerativeAI` is a **Chat Model** (not a plain LLM), so `.invoke()` returns an `AIMessage` object. You need `.content` to extract the text string from it.

### Quick comparison of all 3 ways you now have to talk to Gemini:

```
METHOD 1: Raw Gemini SDK (your Cell 4)
───────────────────────────────────────
  response = model.generate_content("Hello")
  print(response.text)
  
  ↑ Direct API call. NOT compatible with LangChain chains.
  ↑ Equivalent of: model.generate("Hello") in IBM

METHOD 2: LangChain wrapper with .invoke() (what you're asking about now)
──────────────────────────────────────────────────────────────────────────
  response = llm.invoke("Hello")
  print(response.content)         # <-- .content because it returns AIMessage
  
  ↑ LangChain-compatible. CAN be used in chains with |
  ↑ Equivalent of: llama_llm.invoke("Hello") in IBM

METHOD 3: LangChain wrapper in a chain (what you'll do later)
─────────────────────────────────────────────────────────────
  chain = prompt_template | llm | StrOutputParser()
  result = chain.invoke({"topic": "dogs"})
  print(result)                   # <-- already a string, StrOutputParser extracts .content for you
  
  ↑ Full LCEL chain. StrOutputParser handles the .content extraction automatically.
```

So for your Cell 5, you'd write something like:

```python
# Equivalent of: llama_llm = WatsonxLLM(model=model)
# You already have `llm` from Cell 3 — that IS your WatsonxLLM equivalent.
# No extra wrapping step needed with Google!

# Equivalent of: print(llama_llm.invoke("Who is man's best friend?"))
print(llm.invoke("Who is man's best friend?").content)
```

The reason Google doesn't need a separate wrapping step like `WatsonxLLM(model=model)` is that `ChatGoogleGenerativeAI` already **is** the LangChain wrapper -- it handles both the API connection and LangChain compatibility in one object.