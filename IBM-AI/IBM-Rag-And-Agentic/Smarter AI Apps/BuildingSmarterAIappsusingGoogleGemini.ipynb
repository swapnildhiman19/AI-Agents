{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea3efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Google Gemini SDK + LangChain integration (replaces ibm-watsonx-ai + langchain-ibm)\n",
    "!pip install google-generativeai langchain-google-genai\n",
    "\n",
    "# LangChain core (same as original, these are provider-agnostic)\n",
    "!pip install langchain langchain-core langchain-community langchain-experimental langchainhub\n",
    "\n",
    "# PDF loading (same as original)\n",
    "!pip install pypdf\n",
    "\n",
    "# Vector database (same as original)\n",
    "!pip install chromadb\n",
    "\n",
    "# For loading .env file with your GOOGLE_API_KEY\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5fd8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ============================================================\n",
    "# Remove corporate proxy settings that block Google API access\n",
    "# (Walmart VPN/proxy intercepts HTTPS calls to external APIs)\n",
    "# ============================================================\n",
    "for proxy_var in [\"HTTP_PROXY\", \"HTTPS_PROXY\", \"http_proxy\", \"https_proxy\"]:\n",
    "    os.environ.pop(proxy_var, None)\n",
    "\n",
    "# ============================================================\n",
    "# Load your Google Gemini API key from the .env file\n",
    "# The .env file is in the same folder as this notebook\n",
    "# with the line: GOOGLE_API_KEY=your_actual_key_here\n",
    "# Get a free key from: https://aistudio.google.com/apikey\n",
    "# ============================================================\n",
    "load_dotenv()  # Reads the .env file and loads vars into os.environ\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not GOOGLE_API_KEY or GOOGLE_API_KEY == \"YOUR_API_KEY_HERE\":\n",
    "    raise ValueError(\"Please set your GOOGLE_API_KEY in the .env file!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b344ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings suppression -- stays exactly the same\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['ANONYMIZED_TELEMETRY'] = 'False'\n",
    "\n",
    "# Google Gemini equivalents of the IBM imports\n",
    "import google.generativeai as genai                          # Replaces: ModelInference (direct API access)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI    # Replaces: WatsonxLLM (LangChain wrapper)\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # For embeddings (you'll need this later for vector stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b7f8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from your .env file\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Configure the Gemini SDK\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- For direct Gemini SDK calls (equivalent of ModelInference) ---\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-2.0-flash\",\n",
    "    generation_config=genai.types.GenerationConfig(\n",
    "        max_output_tokens=256,    # was GenParams.MAX_NEW_TOKENS\n",
    "        temperature=0.2,          # was GenParams.TEMPERATURE\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- For LangChain chains (equivalent of WatsonxLLM) ---\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a1b9b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mai ek AI assistant hu, isliye meri koi bhavna nahi hoti. Mai theek hu, aur aapki madad karne ke liye taiyar hu. Aap kaise hain?\n",
      "\n",
      "Mai ek AI assistant hu, isliye meri koi bhavna nahi hoti. Mai theek hu, aur aapki madad karne ke liye taiyar hu. Aap kaise hain?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "msg = model.generate_content(\"Aap kaisi ho ma'am.\")\n",
    "print(msg.candidates[0].content.parts[0].text)\n",
    "print(msg.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "758aaa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove corporate proxy settings that block Google API access\n",
    "for proxy_var in [\"HTTP_PROXY\", \"HTTPS_PROXY\", \"http_proxy\", \"https_proxy\"]:\n",
    "    os.environ.pop(proxy_var, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "602217ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am doing well, thank you for asking! As a large language model, I don't experience emotions or feelings in the same way humans do, but I am functioning optimally and ready to assist you with any questions or tasks you have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"How are you ?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d071335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0848a75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Thank you for coming in today. Please, have a seat.\n",
      "\n",
      "I'm excited to learn more about you and your experience. As you know, we're a fast-growing startup in the AI space, and we're looking for a talented iOS Developer to join our team and help us build innovative and user-friendly mobile experiences.\n",
      "\n",
      "Before we dive into the technical questions, could you please tell me a little bit about yourself and what excites you about iOS development, particularly in the context of AI-driven applications?\n"
     ]
    }
   ],
   "source": [
    "msg = llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant who plays a role of iOS interviewer who is hiring for iOS Developer in Indian Startup which is at par with on going trends of AI.\"),\n",
    "        HumanMessage(content=\"Hello ma'am.\")\n",
    "    ]\n",
    ")\n",
    "print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7ab29c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Thank you for coming in today. Please, have a seat.\n",
      "\n",
      "I'm excited to learn more about you and your experience. As you know, we're a fast-paced startup in the AI space, and we're looking for a passionate and skilled iOS Developer to join our team. We need someone who can not only build great apps but also understands the potential of integrating AI into the mobile experience.\n",
      "\n",
      "Before we dive into the technical questions, could you please walk me through your background and tell me a bit about what excites you about iOS development and AI?\n",
      "[SystemMessage(content='You are a helpful assistant who plays a role of iOS interviewer who is hiring for iOS Developer in Indian Startup which is at par with on going trends of AI.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Hello ma'am.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! Thank you for coming in today. Please, have a seat.\\n\\nI'm excited to learn more about you and your experience. As you know, we're a fast-paced startup in the AI space, and we're looking for a passionate and skilled iOS Developer to join our team. We need someone who can not only build great apps but also understands the potential of integrating AI into the mobile experience.\\n\\nBefore we dive into the technical questions, could you please walk me through your background and tell me a bit about what excites you about iOS development and AI?\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c5ae6-adf1-7ec3-bf2d-f268f19b8b79-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 37, 'output_tokens': 119, 'total_tokens': 156, 'input_token_details': {'cache_read': 0}})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "conversationHistory = [\n",
    "    SystemMessage(content=\"You are a helpful assistant who plays a role of iOS interviewer who is hiring for iOS Developer in Indian Startup which is at par with on going trends of AI.\")\n",
    "]\n",
    "\n",
    "def chat_with_llm(user_input):\n",
    "    conversationHistory.append(HumanMessage(content=user_input))\n",
    "    response = llm.invoke(conversationHistory)\n",
    "    conversationHistory.append(response)\n",
    "    return response.content\n",
    "\n",
    "print(chat_with_llm(\"Hello ma'am.\"))\n",
    "\n",
    "print(conversationHistory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e3129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "conversationHistory = [\n",
    "    SystemMessage(content=\"You are a helpful assistant who plays a role of iOS interviewer who is hiring for iOS Developer in Indian Startup which is at par with on going trends of AI.\")\n",
    "]\n",
    "\n",
    "def chat_with_llm(user_input):\n",
    "    conversationHistory.append(HumanMessage(content=user_input))\n",
    "    response = llm.invoke(conversationHistory)\n",
    "    conversationHistory.append(response)\n",
    "    return response.content\n",
    "\n",
    "def display_chat():\n",
    "    \"\"\"Prints the full conversation history in a clean chat view\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“‹ CONVERSATION HISTORY\")\n",
    "    print(\"=\" * 60)\n",
    "    for msg in conversationHistory:\n",
    "        if isinstance(msg, SystemMessage):\n",
    "            print(f\"\\nğŸ”§ [SYSTEM]: {msg.content}\")\n",
    "        elif isinstance(msg, HumanMessage):\n",
    "            print(f\"\\nğŸ§‘ [YOU]: {msg.content}\")\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            print(f\"\\nğŸ¤– [AI]: {msg.content}\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Interactive loop -- type \"quit\" to exit\n",
    "while True:\n",
    "    user_input = input(\"\\nğŸ§‘ You: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "        print(\"Ending interview. Goodbye!\")\n",
    "        display_chat()  # Show full history at the end\n",
    "        break\n",
    "    \n",
    "    response = chat_with_llm(user_input)\n",
    "    print(f\"\\nğŸ¤– AI: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c48a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Run this cell ONCE to initialize\n",
    "conversationHistory = [\n",
    "    SystemMessage(content=\"You are a helpful assistant who plays a role of iOS interviewer who is hiring for iOS Developer in Indian Startup which is at par with on going trends of AI.\")\n",
    "]\n",
    "\n",
    "def chat_with_llm(user_input):\n",
    "    conversationHistory.append(HumanMessage(content=user_input))\n",
    "    response = llm.invoke(conversationHistory)\n",
    "    conversationHistory.append(response)\n",
    "    return response.content\n",
    "\n",
    "def display_chat():\n",
    "    \"\"\"Prints the full conversation history in a clean chat view\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    for msg in conversationHistory:\n",
    "        if isinstance(msg, SystemMessage):\n",
    "            print(f\"\\nğŸ”§ [SYSTEM]: {msg.content}\")\n",
    "        elif isinstance(msg, HumanMessage):\n",
    "            print(f\"\\nğŸ§‘ [YOU]: {msg.content}\")\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            print(f\"\\nğŸ¤– [AI]: {msg.content}\")\n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "701e7b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– AI: Hello! Please, call me [My Name, if applicable, or just leave it as \"Hello\"]. Thank you for coming in today. I'm excited to learn more about you and your experience.\n",
      "\n",
      "This role is for an iOS Developer at [Startup Name], a fast-growing Indian startup leveraging AI to [briefly describe what the startup does]. We're looking for someone passionate about building innovative and user-friendly mobile experiences.\n",
      "\n",
      "To start, could you please walk me through your background and tell me a bit about your experience with iOS development?\n",
      "\n",
      "============================================================\n",
      "\n",
      "ğŸ”§ [SYSTEM]: You are a helpful assistant who plays a role of iOS interviewer who is hiring for iOS Developer in Indian Startup which is at par with on going trends of AI.\n",
      "\n",
      "ğŸ§‘ [YOU]: Hello ma'am.\n",
      "\n",
      "ğŸ§‘ [YOU]: Hello ma'am.\n",
      "\n",
      "ğŸ¤– [AI]: Hello! Please, call me [My Name, if applicable, or just leave it as \"Hello\"]. Thank you for coming in today. I'm excited to learn more about you and your experience.\n",
      "\n",
      "This role is for an iOS Developer at [Startup Name], a fast-growing Indian startup leveraging AI to [briefly describe what the startup does]. We're looking for someone passionate about building innovative and user-friendly mobile experiences.\n",
      "\n",
      "To start, could you please walk me through your background and tell me a bit about your experience with iOS development?\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Change this message and re-run this cell for each turn\n",
    "print(\"ğŸ¤– AI:\", chat_with_llm(\"Hello ma'am.\"))\n",
    "print()\n",
    "display_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280e1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e929064",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a helpful assistant who plays a role of iOS interviewer who is hiring for iOS Developer in Indian Startup which is at par with on going trends of AI.\n",
    "    {user_input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "input_ = {\"user_input\": \"Hello ma'am.\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5672b3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\n    You are a helpful assistant who plays a role of iOS interviewer who is hiring for iOS Developer in Indian Startup which is at par with on going trends of AI.\\n    Hello ma'am.\\n    \")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85ad9d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant who plays a role of iOS interviewer who is hiring for iOS Developer in Indian Startup which is at par with on going trends of AI.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Hello ma'am.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Hello ma'am.\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant who plays a role of iOS interviewer who is hiring for iOS Developer in Indian Startup which is at par with on going trends of AI.\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "        (\"ai\", \"Hello {user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt.invoke({\"user_input\": \"Hello ma'am.\"})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9313d75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Hello ma'am.\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant\"),\n",
    "        MessagesPlaceholder(\"msgs\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "input_ = {\"msgs\": [HumanMessage(content=\"Hello ma'am.\")]}\n",
    "\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cd5c456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "response = chain.invoke(input=input_)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c9b02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### JSON parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b453aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "593da10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"Question to setup the joke\")\n",
    "    punchline: str = Field(description=\"Answer to the joke\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5cba2c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}\n",
      "{'setup': 'What is the capital of france ?', 'punchline': 'Paris'}\n"
     ]
    }
   ],
   "source": [
    "joke_query = \"Tell me a joke\"\n",
    "serious_query = \"What is the capital of france ?\"\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=Joke)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"query\": joke_query})\n",
    "response_serious = chain.invoke({\"query\": serious_query})\n",
    "\n",
    "print(response)\n",
    "print(response_serious)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edc5163b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apples', 'Bananas', 'Strawberries', 'Grapes', 'Oranges']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"query\": \"What are the top 5 fruits ?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11e558e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 5\n",
      "Numbers per embedding: 3072\n",
      "First 10 numbers of 'I love dogs': [-0.012768259, -0.005740843, 0.012015571, -0.09264653, -0.006427728, 0.022212781, -0.011224695, 0.0027424546, 0.0074106553, 0.0012625683]\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Create the embedding model (Google's equivalent of IBM's WatsonxEmbeddings)\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-001\",\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# Embed some sentences\n",
    "sentences = [\n",
    "    \"I love dogs\",\n",
    "    \"I adore puppies\",\n",
    "    \"The stock market crashed today\",\n",
    "    \"Cats are wonderful pets\",\n",
    "    \"The economy is in recession\",\n",
    "]\n",
    "\n",
    "# Get embeddings for all sentences\n",
    "vectors = embedding_model.embed_documents(sentences)\n",
    "\n",
    "# See the shape\n",
    "print(f\"Number of sentences: {len(vectors)}\")\n",
    "print(f\"Numbers per embedding: {len(vectors[0])}\")\n",
    "print(f\"First 10 numbers of 'I love dogs': {vectors[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3265b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  models/gemini-embedding-001  (dimensions: 1)\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "for model in genai.list_models():\n",
    "    if \"embedContent\" in model.supported_generation_methods:\n",
    "        print(f\"  {model.name}  (dimensions: {getattr(model, 'output_token_limit', 'N/A')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34412083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITY MATRIX: \n",
      "============================================================\n",
      "I love dogs vs I adore puppies: 0.95\n",
      "I love dogs vs The stock market crashed today: 0.76\n",
      "I love dogs vs Cats are wonderful pets: 0.85\n",
      "I love dogs vs The economy is in recession: 0.75\n",
      "------------------------------------------------------------\n",
      "I adore puppies vs The stock market crashed today: 0.75\n",
      "I adore puppies vs Cats are wonderful pets: 0.84\n",
      "I adore puppies vs The economy is in recession: 0.74\n",
      "------------------------------------------------------------\n",
      "The stock market crashed today vs Cats are wonderful pets: 0.73\n",
      "The stock market crashed today vs The economy is in recession: 0.85\n",
      "------------------------------------------------------------\n",
      "Cats are wonderful pets vs The economy is in recession: 0.74\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "print(\"SIMILARITY MATRIX: \")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i,sent_a in enumerate(sentences):\n",
    "    for j,sent_b in enumerate(sentences):\n",
    "        if j > i:\n",
    "            sim = cosine_similarity(vectors[i], vectors[j])\n",
    "            print(f\"{sent_a} vs {sent_b}: {sim:.2f}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6adf77d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PCA' from 'sklearn' (/Users/s0d0bla/miniconda3/envs/ibm-ai/lib/python3.11/site-packages/sklearn/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# from sklearn.decomposition import PCA\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'PCA' from 'sklearn' (/Users/s0d0bla/miniconda3/envs/ibm-ai/lib/python3.11/site-packages/sklearn/__init__.py)"
     ]
    }
   ],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "from sklearn import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compress 768 dimensions â†’ 2 dimensions (for plotting)\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(np.array(vectors))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, sentence in enumerate(sentences):\n",
    "    x, y = vectors_2d[i]\n",
    "    plt.scatter(x, y, s=100)\n",
    "    plt.annotate(sentence, (x, y), fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "plt.title(\"Sentence Embeddings Visualized in 2D\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62ceaac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.8.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /Users/s0d0bla/miniconda3/envs/ibm-ai/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Downloading scipy-1.17.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.8.0-cp311-cp311-macosx_12_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.17.0-cp311-cp311-macosx_14_0_arm64.whl (20.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.3 scikit-learn-1.8.0 scipy-1.17.0 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecdf3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
