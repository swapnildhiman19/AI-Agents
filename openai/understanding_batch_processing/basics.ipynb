{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de28a42c",
   "metadata": {},
   "source": [
    "#Understanding how to do batch-processing OpenAI API call:\n",
    "\n",
    "### Layman's Explanation\n",
    "\n",
    "You're right that a batch job involves a massive number of tokens, so the total bill for that single job will be large. However, the key is that you are paying a **much lower price for each token**.\n",
    "\n",
    "Think of it like buying in bulk at a warehouse store versus buying from a local convenience store:\n",
    "*   **Real-time API (Convenience Store):** You buy one item at a time. It's fast and convenient, but you pay the full retail price for each item.\n",
    "*   **Batch API (Warehouse Store):** You buy a huge pallet of items at once. The total bill is high, but the price you pay *per item* is significantly discounted.\n",
    "\n",
    "You would only use the batch method for tasks that are *already* huge (like summarizing 10,000 documents). Doing this one-by-one would be incredibly expensive. Batch processing makes these large-scale tasks financially feasible by giving you a bulk discount.\n",
    "\n",
    "So, while the total token count is high, the **cost per token is much lower**, leading to significant overall savings for that specific, large-scale job.\n",
    "\n",
    "### Deep Dive: Cost Structure of Batch vs. Real-time APIs\n",
    "\n",
    "Let's look at the mechanics of why batch processing is more cost-effective for high-volume tasks, thinking generally across AI platforms like Anthropic, Google's Vertex AI, and others that offer this feature.\n",
    "\n",
    "#### 1. Per-Token Pricing Discount\n",
    "\n",
    "The most significant factor is the **direct discount on the per-token price**. AI providers can process batch jobs more efficiently on their end by scheduling them during off-peak hours and optimizing hardware usage. They pass these savings on to you.\n",
    "\n",
    "*   **Real-time (Synchronous) Calls:** You pay the standard, premium rate for an instant response.\n",
    "*   **Batch (Asynchronous) Calls:** You typically receive a **discount of around 50%** on the per-token cost for both input and output.\n",
    "\n",
    "**Hypothetical Cost Comparison:**\n",
    "\n",
    "Let's say the standard rate for a model is **$3.00 per million input tokens** and **$15.00 per million output tokens**.\n",
    "\n",
    "| Task | Real-time (Synchronous) API Cost | Batch API Cost (with 50% Discount) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Input Tokens** | $3.00 / 1M tokens | **$1.50 / 1M tokens** |\n",
    "| **Output Tokens**| $15.00 / 1M tokens| **$7.50 / 1M tokens** |\n",
    "\n",
    "For a task involving 10 million input tokens and 2 million output tokens:\n",
    "*   **Real-time Cost:** (10 * $3.00) + (2 * $15.00) = $30 + $30 = **$60.00**\n",
    "*   **Batch Cost:** (10 * $1.50) + (2 * $7.50) = $15 + $15 = **$30.00**\n",
    "\n",
    "You save **50%** by using the batch API.\n",
    "\n",
    "#### 2. Reduced Operational and Network Overhead\n",
    "\n",
    "Beyond the token price, batching reduces other costs associated with making thousands of individual API calls:\n",
    "\n",
    "*   **Fewer API Calls:** Instead of managing 10,000 separate HTTP requests, responses, and potential retries, you make just a few calls: one to upload the batch file, one to start the job, and one to download the results.\n",
    "*   **Simplified Code:** Your application logic becomes much simpler. You don't need complex loops, error handling for individual failed requests, or rate-limit management. This saves development and maintenance time, which translates to lower operational costs.\n",
    "*   **No Real-time Infrastructure Needed:** You don't need to maintain a system that can handle thousands of concurrent real-time connections, which can be expensive to scale.\n",
    "\n",
    "#### 3. Use Case Alignment\n",
    "\n",
    "Batch processing is not meant for every task. It's specifically designed for workloads that are **inherently large-scale and not time-sensitive**.\n",
    "\n",
    "*   If you have a task that requires processing 10,000 documents, that task will have a high token count *no matter how you do it*.\n",
    "*   The choice is not between a \"low token\" method and a \"high token\" method. The choice is between an **expensive way** (real-time) and a **cheap way** (batch) to process the *same number of tokens*.\n",
    "\n",
    "### Summary Table: Cost-Effectiveness of Batch Messaging\n",
    "\n",
    "| Aspect | Real-time (Synchronous) API | Batch API | Why Batch is More Cost-Effective |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Token Price** | Standard, premium rate. | **Significantly discounted** (e.g., 50% off). | The core reason for direct cost savings. |\n",
    "| **Total Cost for Large Jobs** | High, due to standard pricing. | Lower, due to the bulk discount. | It's the most economical option for large-scale work. |\n",
    "| **Operational Complexity** | High (managing many calls, errors, rate limits). | Low (a few API calls to manage the entire job). | Saves development time and reduces infrastructure costs. |\n",
    "| **Best For** | Interactive, time-sensitive tasks. | High-volume, non-urgent tasks. | The tool is designed for cost efficiency at scale. |\n",
    "\n",
    "In conclusion, while a batch job processes a large volume of tokens and results in a single, large bill, it is the most **cost-effective method available** for that specific type of high-volume workload because of the substantial per-token discounts and reduced operational overhead.\n",
    "\n",
    "Excellent question. You've hit on one of the most common and ideal use cases for batch processing. Here’s a clear explanation of why batching is the right approach for creating embeddings for a vector database.\n",
    "\n",
    "### **Layman's Explanation**\n",
    "\n",
    "Yes, absolutely. For creating a vector database from a large amount of your personal data, **batch processing is the perfect tool for the job.**\n",
    "\n",
    "Think of it this way:\n",
    "*   **Your Goal:** To teach a new AI assistant about all your documents. To do this, you first need to convert every document into a special numerical \"summary\" (an embedding) that the AI can understand.\n",
    "*   **The Task:** You have a large pile of documents (your personal data) that all need this conversion.\n",
    "*   **The Two Ways to Do It:**\n",
    "    1.  **Real-time Way:** You could feed the documents to the conversion machine one by one, wait for each one to finish, and then do the next. This is slow and you'd be paying the full price for every single conversion.\n",
    "    2.  **Batch Way:** You put all your documents into a single big box, give it to the machine, and say, \"Convert all of these when you have time.\" The machine works on the whole box overnight and gives you all the converted summaries back in the morning.\n",
    "\n",
    "For building a vector database, the **batch way is much better**. You don't need the embeddings instantly, and you get a huge \"bulk discount\" for processing everything at once, making it faster and much cheaper.\n",
    "\n",
    "### **Deep Dive: Batch Processing for Vector Embeddings**\n",
    "\n",
    "You are correct to connect these two concepts. Creating embeddings for a large corpus of documents to populate a vector database is a prime example of a high-volume, asynchronous task where batch APIs excel.\n",
    "\n",
    "#### **What is an Embedding and a Vector Database?**\n",
    "\n",
    "1.  **Embedding:** An embedding is a numerical representation (a vector) of a piece of text (like a sentence, paragraph, or document). This vector captures the text's semantic meaning, allowing computers to understand relationships and similarities between different pieces of text.\n",
    "2.  **Vector Database:** A specialized database designed to store and efficiently search through these numerical vectors. It's the core component of modern search systems and Retrieval-Augmented Generation (RAG) applications, as it allows you to quickly find the most relevant document chunks to answer a user's query.\n",
    "\n",
    "#### **Why is Batch Processing the Ideal Method for this?**\n",
    "\n",
    "When you are first setting up your vector database, you need to process all of your existing documents. This is a large, one-time \"bulk\" operation. Here’s why the batch API is the superior choice:\n",
    "\n",
    "1.  **Massive Cost Savings:** Embedding generation is priced per token. Batch APIs often provide a **significant discount (typically around 50%)** compared to real-time (synchronous) APIs. For a large dataset (e.g., gigabytes of text), this can translate into saving hundreds or thousands of dollars.\n",
    "2.  **High Throughput and Rate Limits:** Real-time APIs have strict rate limits (e.g., requests per minute). Trying to embed thousands of documents one by one would quickly hit these limits, forcing you to add complex logic for delays and retries. Batch APIs are designed for high throughput, allowing you to submit a job with hundreds of thousands of documents at once.\n",
    "3.  **Asynchronous by Nature:** Populating a vector database is not a real-time task. You can start the embedding process and let it run in the background. The asynchronous nature of batch APIs is a perfect fit—you submit the job, go do something else, and come back later to retrieve the results.\n",
    "4.  **Simplified Workflow:** Instead of writing a script to loop through each document, make an individual API call, handle errors, and manage rate limits, your workflow becomes much simpler:\n",
    "    *   **Prepare:** Create a single file containing all your document chunks.\n",
    "    *   **Submit:** Make one API call to upload the file and start the batch job.\n",
    "    *   **Retrieve:** Make one API call to download the completed file of embeddings.\n",
    "\n",
    "#### **When Would You NOT Use Batch Processing for Embeddings?**\n",
    "\n",
    "While batching is perfect for the initial bulk load, you would use a **real-time (synchronous) API call** in one specific scenario:\n",
    "\n",
    "*   **Real-time Indexing:** When a *new* piece of data is added to your system (e.g., a user uploads a new document) and you want it to be searchable immediately. In this case, you would make a single, synchronous API call to generate the embedding for that one document and insert it into your vector database right away.\n",
    "\n",
    "### **Summary Table: Choosing the Right API for Embeddings**\n",
    "\n",
    "| Feature | Batch API (for Bulk Loading) | Real-time (Synchronous) API (for Updates) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Use Case** | Initial creation of a vector database from a large corpus of documents. | Adding a single new document to an existing database for immediate searchability. |\n",
    "| **Cost** | **Significantly cheaper** (e.g., 50% discount per token). | Standard, premium pricing. |\n",
    "| **Performance** | High throughput, designed for millions of documents. | Low latency, designed for single, quick requests. |\n",
    "| **Complexity** | Simple workflow: prepare, submit, retrieve. | More complex for bulk tasks (requires loops, error handling, rate limiting). |\n",
    "| **Ideal For** | Large, non-urgent, one-time processing tasks. | Small, time-sensitive, interactive tasks. |\n",
    "\n",
    "**Conclusion:** For your goal of creating a vector database from your personal data, opting for a **Batch Processing AI API is unequivocally the correct, most efficient, and most cost-effective strategy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a97d8f",
   "metadata": {},
   "source": [
    "Step 1: Setting Up Your Environment and API Key\n",
    "First, make sure you have the necessary library installed and your API key is set up in a .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed3a5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"OpenAI client initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fabf41c",
   "metadata": {},
   "source": [
    "Step 2: Prepare Your Batch Input File (JSONL Format)\n",
    "The batch API requires a .jsonl (JSON Lines) file, where each line is a separate JSON object representing a single API call.\n",
    "\n",
    "custom_id: A unique ID you create to match a request to its response later.\n",
    "\n",
    "method: Must be POST.\n",
    "\n",
    "url: For chat completions, this is /v1/chat/completions.\n",
    "\n",
    "body: This contains the model and messages, just like a normal API call.\n",
    "\n",
    "Here’s the Python code to create this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080704b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input file 'batch_prompts.jsonl' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the prompts for our batch job\n",
    "batch_prompts = [\n",
    "    {\"custom_id\": \"request-1\", \"review_text\": \"I loved the product! It's fantastic.\"},\n",
    "    {\"custom_id\": \"request-2\", \"review_text\": \"The shipping was too slow and the box was damaged.\"},\n",
    "    {\"custom_id\": \"request-3\", \"review_text\": \"It's an okay product, not great but not terrible either.\"},\n",
    "]\n",
    "\n",
    "# Name of the file we will create\n",
    "batch_input_file_name = \"batch_prompts.jsonl\"\n",
    "\n",
    "# Create the JSONL file\n",
    "with open(batch_input_file_name, \"w\") as f:\n",
    "    for job in batch_prompts:\n",
    "        json_string = json.dumps({\n",
    "            \"custom_id\": job[\"custom_id\"],\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a sentiment analysis expert. Classify the following customer review as Positive, Negative, or Neutral.\"},\n",
    "                    {\"role\": \"user\", \"content\": job[\"review_text\"]}\n",
    "                ],\n",
    "                \"max_tokens\": 10\n",
    "            }\n",
    "        })\n",
    "        f.write(json_string + \"\\n\")\n",
    "\n",
    "print(f\"Batch input file '{batch_input_file_name}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9322cf73",
   "metadata": {},
   "source": [
    "Step 3: Upload the File to OpenAI\n",
    "Now, we upload the file we just created. OpenAI will give us back a file_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4afddb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded successfully. File ID: file-2MSu5nPCa4QFcbkmQtsZYM\n"
     ]
    }
   ],
   "source": [
    "# Upload the file to OpenAI\n",
    "batch_file = client.files.create(\n",
    "    file=open(batch_input_file_name, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "# Store the file ID\n",
    "batch_file_id = batch_file.id\n",
    "\n",
    "print(f\"File uploaded successfully. File ID: {batch_file_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffcb403",
   "metadata": {},
   "source": [
    "Step 4: Create and Run the Batch Job\n",
    "Using the file_id, we can now tell OpenAI to start processing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "261d135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch job created successfully. Job ID: batch_687a56e58c748190a243fa4807a4ea18\n"
     ]
    }
   ],
   "source": [
    "# Create the batch job\n",
    "batch_job = client.batches.create(\n",
    "    input_file_id=batch_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\" # The job must be completed within 1 hour\n",
    ")\n",
    "\n",
    "# Store the job ID\n",
    "batch_job_id = batch_job.id\n",
    "\n",
    "print(f\"Batch job created successfully. Job ID: {batch_job_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce44a8",
   "metadata": {},
   "source": [
    "Step 5: Check the Status and Retrieve the Results\n",
    "A batch job is asynchronous, so it won't be done instantly. We need to check its status periodically until it's completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4f4686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for batch job to complete...\n",
      "Current job status: in_progress\n",
      "Current job status: in_progress\n",
      "Current job status: in_progress\n",
      "Current job status: in_progress\n",
      "Current job status: in_progress\n",
      "Current job status: in_progress\n",
      "Current job status: in_progress\n",
      "Current job status: in_progress\n",
      "Current job status: in_progress\n",
      "Current job status: in_progress\n",
      "Current job status: in_progress\n",
      "Current job status: in_progress\n",
      "Current job status: in_progress\n",
      "Current job status: completed\n",
      "Batch job completed!\n",
      "Results downloaded and saved to 'batch_results.jsonl'.\n",
      "Result for request-1: Positive\n",
      "Result for request-2: Negative\n",
      "Result for request-3: Neutral\n"
     ]
    }
   ],
   "source": [
    "# Wait for the batch job to complete\n",
    "print(\"Waiting for batch job to complete...\")\n",
    "while True:\n",
    "    batch_job = client.batches.retrieve(batch_job_id)\n",
    "    print(f\"Current job status: {batch_job.status}\")\n",
    "    \n",
    "    if batch_job.status == \"completed\":\n",
    "        break\n",
    "    elif batch_job.status in [\"failed\", \"expired\", \"cancelling\", \"cancelled\"]:\n",
    "        print(\"Job failed or was cancelled.\")\n",
    "        exit() # Exit if the job fails\n",
    "        \n",
    "    time.sleep(10) # Wait 10 seconds before checking again\n",
    "\n",
    "print(\"Batch job completed!\")\n",
    "\n",
    "# Retrieve the results\n",
    "if batch_job.status == \"completed\":\n",
    "    output_file_id = batch_job.output_file_id\n",
    "    result_content = client.files.content(output_file_id).read()\n",
    "    \n",
    "    # Save the results to a local file\n",
    "    with open(\"batch_results.jsonl\", \"wb\") as f:\n",
    "        f.write(result_content)\n",
    "        \n",
    "    print(\"Results downloaded and saved to 'batch_results.jsonl'.\")\n",
    "    \n",
    "    # Print the results\n",
    "    results_data = result_content.decode('utf-8').strip().split('\\n')\n",
    "    for line in results_data:\n",
    "        data = json.loads(line)\n",
    "        custom_id = data['custom_id']\n",
    "        response_body = data['response']['body']\n",
    "        content = response_body['choices'][0]['message']['content']\n",
    "        print(f\"Result for {custom_id}: {content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79b6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
