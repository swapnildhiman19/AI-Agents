{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "132f070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch transformers huggingface_hub requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f682af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/s0d0bla/miniconda3/envs/hello/lib/python3.10/site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6c0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "978ed3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# # Fetch HF_TOKEN\n",
    "# hf_token = os.environ.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c3d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import requests\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4d3174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyAZScbVXfg6EBoUIU7xRZnrsqUFDX8dOFs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load your Gemini API key from .env or environment variable\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "print(GEMINI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_llm(messages, model=\"gemini-1.5-pro-latest\"):\n",
    "    \"\"\"\n",
    "    Calls Gemini API with a list of messages (for chat).\n",
    "    Returns the model's response as a string.\n",
    "    \"\"\"\n",
    "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={GEMINI_API_KEY}\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"contents\": messages,\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": 0.3,\n",
    "            \"maxOutputTokens\": 1024\n",
    "        }\n",
    "    }\n",
    "    resp = requests.post(url, headers=headers, json=data)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4b5dc",
   "metadata": {},
   "source": [
    "Authenticate with Hugging Face (Gemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b556b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff83197",
   "metadata": {},
   "source": [
    "# Loading a Hugging Face Model : Gemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f135022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model_name = \"google/gemma-2b-it\" #This is a big model and codespace is not able to run it, replacing it with a smaller one\n",
    "# model_name = \"microsoft/phi-3-mini-4k-instruct\"  # A smaller model for demonstration\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "# # Downloads and loads the tokenizer for DistilGPT2. \n",
    "# # The tokenizer converts raw text into tokens (numbers) that the model can understand. \n",
    "# # It uses the same byte-level BPE as GPT-2. This is where the text is converted to embeddings (indirectly): the tokenizer outputs token IDs,\n",
    "# # which the model then turns into embeddings in its first layer.\n",
    "\n",
    "# # Downloads and loads the pre-trained DistilGPT2 model. This is the neural network that generates text, given the input tokens.\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, token=hf_token)\n",
    "\n",
    "# #The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.\n",
    "# # Pipeline we get from Hugging Face, text-generation parameter should be compatible with the model we are using.\n",
    "# #Wraps the model and tokenizer in a high-level pipeline for easy text generation. Now you can just call llm(\"Your prompt\") to generate text. max_new_tokens=256 limits the output length.\n",
    "# llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)\n",
    "\n",
    "\n",
    "# Install OpenAI Python client if not already installed\n",
    "# pip install openai\n",
    "\n",
    "# import openai\n",
    "# from openai import OpenAI\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# # Fetch OpenAI API key\n",
    "# openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# # Set up OpenAI client\n",
    "# client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# def openai_llm(prompt, model=\"gpt-3.5-turbo\", max_tokens=256):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=model,\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         max_tokens=max_tokens,\n",
    "#         temperature=0\n",
    "#     )\n",
    "#     return response.choices[0].message.content.strip()\n",
    "\n",
    "# # Use this function as llm in your ReAct loop\n",
    "# llm = openai_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b267d8",
   "metadata": {},
   "source": [
    "# Weather tooling function : Our function to get the weather. Ideally our LLM Model should be able to call this function according to the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4feb9f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Weather Tool Function (using Open-Meteo, free, no auth needed) ---\n",
    "\n",
    "def get_weather(city):\n",
    "    print(f\"Fetching weather for {city}...\")  # Debugging statement\n",
    "    # Geocode city to lat/lon (using Open-Meteo's geocoding)\n",
    "    geo = requests.get(f\"https://geocoding-api.open-meteo.com/v1/search?name={city}\").json()\n",
    "    '''\n",
    "    - Defines a function `get_weather` that takes a city name as input.\n",
    "    - Sends a GET request to the Open-Meteo geocoding API to find the latitude and longitude of the city. The response is converted to JSON.\n",
    "    '''\n",
    "\n",
    "    if not geo.get(\"results\"):\n",
    "        return f\"Could not find location for {city}.\"\n",
    "    '''- Checks if the API response contains a `\"results\"` key. If not, it returns a message saying the location couldn't be found.'''\n",
    "\n",
    "    lat, lon = geo[\"results\"][0][\"latitude\"], geo[\"results\"][0][\"longitude\"]\n",
    "\n",
    "    # Get weather, Sends another GET request to the Open-Meteo weather API, using the latitude and longitude found earlier, to get the current weather. The response is converted to JSON.\n",
    "    weather = requests.get(\n",
    "        f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true\"\n",
    "    ).json()\n",
    "    if \"current_weather\" in weather:\n",
    "        temp = weather[\"current_weather\"][\"temperature\"]\n",
    "        desc = f\"The current temperature in {city} is {temp}°C.\"\n",
    "        return desc\n",
    "    else:\n",
    "        return \"Weather data not available.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "496ee5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching weather for Bangalore...\n",
      "Weather Tool Ready. Example usage: The current temperature in Bangalore is 23.2°C.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weather Tool Ready. Example usage: {get_weather('Bangalore')}\") # Testing the weather function: Weather Tool Ready. Example usage: The current temperature in Bangalore is 27.0°C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c61a4",
   "metadata": {},
   "source": [
    "# React Prompting : Reasong and Acting :\n",
    "\n",
    "Here’s a clear explanation of **ReAct prompting**—first in layman’s terms, then in technical detail, with a practical example showing how prompts and model responses work at each step.\n",
    "\n",
    "## **Layman’s Explanation**\n",
    "\n",
    "**ReAct prompting** stands for **Reasoning and Acting**.  \n",
    "It’s a way to get AI to not just “think” (reason) about a problem, but also “do” (act) something step by step—just like a person solving a puzzle:\n",
    "\n",
    "- **Reason:** The AI explains what it’s thinking or planning.\n",
    "- **Act:** The AI takes an action (like looking something up, using a tool, or searching a database).\n",
    "- **Observe:** The AI sees what happened after the action.\n",
    "- **Repeat:** The AI reasons again, takes another action if needed, and continues until it finds the answer.\n",
    "\n",
    "**Why use it?**  \n",
    "It helps the AI solve complex problems more accurately, transparently, and flexibly—especially when it needs to interact with tools or external information[1][2][3][4][5].\n",
    "\n",
    "## **Technical Deep Dive**\n",
    "\n",
    "### **What is ReAct Prompting?**\n",
    "\n",
    "- **ReAct** is a prompting technique that combines:\n",
    "  - **Reasoning:** The AI breaks down the problem, thinks through possible solutions, and plans what to do next.\n",
    "  - **Acting:** The AI takes concrete steps (actions) based on its reasoning, such as searching, calculating, or calling a function.\n",
    "- This process is **iterative**: The AI alternates between reasoning and acting, using the outcome of each action to inform its next step[1][2][3][4][5].\n",
    "\n",
    "### **Key Benefits**\n",
    "\n",
    "- **Transparency:** You can see the AI’s thought process.\n",
    "- **Accuracy:** The AI checks its own work at each step.\n",
    "- **Adaptability:** The AI can adjust its approach if new information appears.\n",
    "- **Reduces Hallucination:** By grounding actions in real observations, the AI is less likely to make things up[1][4][5].\n",
    "\n",
    "### **How ReAct Prompting Works (Step-by-Step Example)**\n",
    "\n",
    "**Scenario:**  \n",
    "Suppose you want the AI to answer:  \n",
    "*\"Who is the current president of the United States, and what is their age?\"*\n",
    "\n",
    "**Prompt Structure:**  \n",
    "You instruct the AI to alternate between Thought (reason), Action (do), and Observation (see result).\n",
    "\n",
    "#### **Step-by-Step Example**\n",
    "\n",
    "**User Prompt:**  \n",
    "```\n",
    "Answer the question step by step. For each step, alternate between:\n",
    "- Thought: What should you do next?\n",
    "- Action: Take an action (e.g., search, calculate).\n",
    "- Observation: What did you find?\n",
    "Repeat until you can answer the question.\n",
    "Question: Who is the current president of the United States, and what is their age?\n",
    "```\n",
    "\n",
    "**Model Response:**\n",
    "\n",
    "1. **Thought:** I need to find out who the current president of the United States is.\n",
    "2. **Action:** Search[\"current president of the United States\"]\n",
    "3. **Observation:** The search result says \"Joe Biden is the current president.\"\n",
    "4. **Thought:** Now I need to find out Joe Biden's age.\n",
    "5. **Action:** Search[\"Joe Biden age\"]\n",
    "6. **Observation:** The search result says \"Joe Biden is 82 years old.\"\n",
    "7. **Final Answer:** The current president of the United States is Joe Biden, who is 82 years old.\n",
    "\n",
    "### **How Is This Used Further?**\n",
    "\n",
    "- **In AI agents:** Each action could be a real API call, database query, or tool use.\n",
    "- **In customer support:** The AI could reason about a complaint, look up order status, and recommend solutions.\n",
    "- **In research:** The AI could plan a series of searches, summarize findings, and synthesize an answer.\n",
    "\n",
    "## **Summary Table**\n",
    "\n",
    "| Step       | What Happens                                    | Example in Practice                                 |\n",
    "|------------|-------------------------------------------------|-----------------------------------------------------|\n",
    "| Thought    | AI decides what to do next                      | \"I need to find the president's name.\"              |\n",
    "| Action     | AI performs an action (search, tool, etc.)      | Search[\"current president of the US\"]               |\n",
    "| Observation| AI sees the result of the action                | \"Result: Joe Biden is the president.\"               |\n",
    "| Repeat     | AI reasons again, takes next action if needed   | \"Now, find Joe Biden's age.\"                        |\n",
    "| Final Answer| AI gives the complete answer                   | \"Joe Biden, age 82.\"                                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c940169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 4. ReAct Loop Implementation ---\n",
    "# def react_loop(user_question):\n",
    "#     history = []  # To store the AI's reasoning and actions\n",
    "#     observation = \"\"\n",
    "#     max_steps = 5  # Prevent infinite loops\n",
    "#     for step in range(max_steps):\n",
    "#         # 1. Build prompt with history, user question, and last observation\n",
    "#         prompt = (\n",
    "#             \"You are an assistant that can reason and use tools.\\n\"\n",
    "#             \"When you need to use a tool, write: Action: get_weather(city)\\n\"\n",
    "#             \"I will execute the action and return the result as Observation.\\n\"\n",
    "#             \"Repeat Thought/Action/Observation until you can answer.\\n\"\n",
    "#             f\"User question: {user_question}\\n\"\n",
    "#         )\n",
    "#         for entry in history:\n",
    "#             prompt += entry + \"\\n\"\n",
    "#         if observation:\n",
    "#             prompt += f\"Observation: {observation}\\n\"\n",
    "\n",
    "#         prompt += \"Thought:\"\n",
    "\n",
    "#         # # 2. Get AI's next step\n",
    "#         # response = llm(prompt, do_sample=False)[0]['generated_text']\n",
    "#         # # Extract only the new completion after the last prompt\n",
    "#         # next_line = response[len(prompt):].strip().split(\"\\n\")[0]\n",
    "#         response = llm(prompt)\n",
    "#         # Extract only the new completion after the last prompt\n",
    "#         next_line = response.strip().split(\"\\n\")[0]\n",
    "\n",
    "#         print(f\"AI: {next_line}\")\n",
    "\n",
    "#         # 3. Check if AI wants to act\n",
    "#         if \"Action:\" in next_line:\n",
    "#             # Parse city from action\n",
    "#             import re\n",
    "#             match = re.search(r'get_weather\\((.*?)\\)', next_line)\n",
    "#             if match:\n",
    "#                 city = match.group(1).strip().strip('\"').strip(\"'\")\n",
    "#                 observation = get_weather(city)\n",
    "#                 history.append(f\"Thought:{next_line}\")\n",
    "#                 history.append(f\"Action: get_weather({city})\")\n",
    "#                 history.append(f\"Observation: {observation}\")\n",
    "#             else:\n",
    "#                 observation = \"Invalid action format.\"\n",
    "#                 history.append(f\"Thought:{next_line}\")\n",
    "#                 history.append(\"Action: none\")\n",
    "#                 history.append(f\"Observation: {observation}\")\n",
    "#         elif \"Final Answer:\" in next_line or \"Answer:\" in next_line:\n",
    "#             print(next_line)\n",
    "#             break\n",
    "#         else:\n",
    "#             # If just reasoning, add to history\n",
    "#             history.append(f\"Thought:{next_line}\")\n",
    "\n",
    "def react_loop(user_question):\n",
    "    history = []\n",
    "    observation = \"\"\n",
    "    max_steps = 5\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Build the prompt as a chat history\n",
    "        prompt = (\n",
    "            \"You are an AI assistant that can reason step by step and use tools.\\n\"\n",
    "            \"When you need to use a tool, write: Action: get_weather(city)\\n\"\n",
    "            \"I will execute the action and return the result as Observation.\\n\"\n",
    "            \"Repeat Thought/Action/Observation until you can answer.\\n\"\n",
    "            f\"User question: {user_question}\\n\"\n",
    "        )\n",
    "        for entry in history:\n",
    "            prompt += entry + \"\\n\"\n",
    "        if observation:\n",
    "            prompt += f\"Observation: {observation}\\n\"\n",
    "        prompt += \"Thought:\"\n",
    "\n",
    "        # Gemini expects a list of messages (chat format)\n",
    "        messages = [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}]\n",
    "        response = gemini_llm(messages)\n",
    "        print(f\"AI: {response}\")\n",
    "\n",
    "        # Parse for Action\n",
    "        if \"Action:\" in response:\n",
    "            import re\n",
    "            import time # Import time for delay between requests\n",
    "            matches = re.findall(r'get_weather\\((.*?)\\)', response)\n",
    "            for city in matches:\n",
    "                time.sleep(2)  # Wait 2 seconds between requests to avoid rate limiting\n",
    "                # Clean up the city name\n",
    "                city = city.strip().strip('\"').strip(\"'\")\n",
    "                #print(f\"Fetching weather for {city}...\")  # Debug\n",
    "                observation = get_weather(city)\n",
    "                history.append(f\"Action: get_weather({city})\")\n",
    "                history.append(f\"Observation: {observation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "492dd5a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=AIzaSyAZScbVXfg6EBoUIU7xRZnrsqUFDX8dOFs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     user_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsk a weather-related question: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mreact_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_q\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 77\u001b[0m, in \u001b[0;36mreact_loop\u001b[0;34m(user_question)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Gemini expects a list of messages (chat format)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]}]\n\u001b[0;32m---> 77\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgemini_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Parse for Action\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mgemini_llm\u001b[0;34m(messages, model)\u001b[0m\n\u001b[1;32m      8\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerationConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     }\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     15\u001b[0m resp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidates\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/hello/lib/python3.10/site-packages/requests/models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1023\u001b[0m     )\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=AIzaSyAZScbVXfg6EBoUIU7xRZnrsqUFDX8dOFs"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_q = input(\"Ask a weather-related question: \")\n",
    "    react_loop(user_q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
