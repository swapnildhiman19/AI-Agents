{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "132f070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch transformers huggingface_hub requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26f682af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/s0d0bla/miniconda3/envs/hello/lib/python3.10/site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d6c0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "978ed3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# # Fetch HF_TOKEN\n",
    "# hf_token = os.environ.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49c3d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import requests\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load your Gemini API key from .env or environment variable\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e18e8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_llm(messages, model=\"gemini-1.5-flash-latest\"):\n",
    "    \"\"\"\n",
    "    Calls Gemini API with a list of messages (for chat).\n",
    "    Returns the model's response as a string.\n",
    "    \"\"\"\n",
    "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={GEMINI_API_KEY}\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"contents\": messages,\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": 0.3,\n",
    "            \"maxOutputTokens\": 1024\n",
    "        }\n",
    "    }\n",
    "    resp = requests.post(url, headers=headers, json=data)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4b5dc",
   "metadata": {},
   "source": [
    "Authenticate with Hugging Face (Gemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83b556b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff83197",
   "metadata": {},
   "source": [
    "# Loading a Hugging Face Model : Gemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f135022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model_name = \"google/gemma-2b-it\" #This is a big model and codespace is not able to run it, replacing it with a smaller one\n",
    "# model_name = \"microsoft/phi-3-mini-4k-instruct\"  # A smaller model for demonstration\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "# # Downloads and loads the tokenizer for DistilGPT2. \n",
    "# # The tokenizer converts raw text into tokens (numbers) that the model can understand. \n",
    "# # It uses the same byte-level BPE as GPT-2. This is where the text is converted to embeddings (indirectly): the tokenizer outputs token IDs,\n",
    "# # which the model then turns into embeddings in its first layer.\n",
    "\n",
    "# # Downloads and loads the pre-trained DistilGPT2 model. This is the neural network that generates text, given the input tokens.\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, token=hf_token)\n",
    "\n",
    "# #The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.\n",
    "# # Pipeline we get from Hugging Face, text-generation parameter should be compatible with the model we are using.\n",
    "# #Wraps the model and tokenizer in a high-level pipeline for easy text generation. Now you can just call llm(\"Your prompt\") to generate text. max_new_tokens=256 limits the output length.\n",
    "# llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)\n",
    "\n",
    "\n",
    "# Install OpenAI Python client if not already installed\n",
    "# pip install openai\n",
    "\n",
    "# import openai\n",
    "# from openai import OpenAI\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# # Fetch OpenAI API key\n",
    "# openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# # Set up OpenAI client\n",
    "# client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# def openai_llm(prompt, model=\"gpt-3.5-turbo\", max_tokens=256):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=model,\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         max_tokens=max_tokens,\n",
    "#         temperature=0\n",
    "#     )\n",
    "#     return response.choices[0].message.content.strip()\n",
    "\n",
    "# # Use this function as llm in your ReAct loop\n",
    "# llm = openai_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b267d8",
   "metadata": {},
   "source": [
    "# Weather tooling function : Our function to get the weather. Ideally our LLM Model should be able to call this function according to the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb9f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Weather Tool Function (using Open-Meteo, free, no auth needed) ---\n",
    "\n",
    "def get_weather(city):\n",
    "    print(f\"Fetching weather for {city}...\")  # Debugging statement\n",
    "    # Geocode city to lat/lon (using Open-Meteo's geocoding)\n",
    "    geo = requests.get(f\"https://geocoding-api.open-meteo.com/v1/search?name={city}\").json()\n",
    "    '''\n",
    "    - Defines a function `get_weather` that takes a city name as input.\n",
    "    - Sends a GET request to the Open-Meteo geocoding API to find the latitude and longitude of the city. The response is converted to JSON.\n",
    "    '''\n",
    "\n",
    "    if not geo.get(\"results\"):\n",
    "        return f\"Could not find location for {city}.\"\n",
    "    '''- Checks if the API response contains a `\"results\"` key. If not, it returns a message saying the location couldn't be found.'''\n",
    "\n",
    "    lat, lon = geo[\"results\"][0][\"latitude\"], geo[\"results\"][0][\"longitude\"]\n",
    "\n",
    "    # Get weather, Sends another GET request to the Open-Meteo weather API, using the latitude and longitude found earlier, to get the current weather. The response is converted to JSON.\n",
    "    weather = requests.get(\n",
    "        f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true\"\n",
    "    ).json()\n",
    "    if \"current_weather\" in weather:\n",
    "        temp = weather[\"current_weather\"][\"temperature\"]\n",
    "        desc = f\"The current temperature in {city} is {temp}°C.\"\n",
    "        return desc\n",
    "    else:\n",
    "        return \"Weather data not available.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "496ee5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching weather for Bangalore...\n",
      "Weather Tool Ready. Example usage: The current temperature in Bangalore is 22.9°C.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weather Tool Ready. Example usage: {get_weather('Bangalore')}\") # Testing the weather function: Weather Tool Ready. Example usage: The current temperature in Bangalore is 27.0°C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c61a4",
   "metadata": {},
   "source": [
    "# React Prompting : Reasong and Acting :\n",
    "\n",
    "Here’s a clear explanation of **ReAct prompting**—first in layman’s terms, then in technical detail, with a practical example showing how prompts and model responses work at each step.\n",
    "\n",
    "## **Layman’s Explanation**\n",
    "\n",
    "**ReAct prompting** stands for **Reasoning and Acting**.  \n",
    "It’s a way to get AI to not just “think” (reason) about a problem, but also “do” (act) something step by step—just like a person solving a puzzle:\n",
    "\n",
    "- **Reason:** The AI explains what it’s thinking or planning.\n",
    "- **Act:** The AI takes an action (like looking something up, using a tool, or searching a database).\n",
    "- **Observe:** The AI sees what happened after the action.\n",
    "- **Repeat:** The AI reasons again, takes another action if needed, and continues until it finds the answer.\n",
    "\n",
    "**Why use it?**  \n",
    "It helps the AI solve complex problems more accurately, transparently, and flexibly—especially when it needs to interact with tools or external information[1][2][3][4][5].\n",
    "\n",
    "## **Technical Deep Dive**\n",
    "\n",
    "### **What is ReAct Prompting?**\n",
    "\n",
    "- **ReAct** is a prompting technique that combines:\n",
    "  - **Reasoning:** The AI breaks down the problem, thinks through possible solutions, and plans what to do next.\n",
    "  - **Acting:** The AI takes concrete steps (actions) based on its reasoning, such as searching, calculating, or calling a function.\n",
    "- This process is **iterative**: The AI alternates between reasoning and acting, using the outcome of each action to inform its next step[1][2][3][4][5].\n",
    "\n",
    "### **Key Benefits**\n",
    "\n",
    "- **Transparency:** You can see the AI’s thought process.\n",
    "- **Accuracy:** The AI checks its own work at each step.\n",
    "- **Adaptability:** The AI can adjust its approach if new information appears.\n",
    "- **Reduces Hallucination:** By grounding actions in real observations, the AI is less likely to make things up[1][4][5].\n",
    "\n",
    "### **How ReAct Prompting Works (Step-by-Step Example)**\n",
    "\n",
    "**Scenario:**  \n",
    "Suppose you want the AI to answer:  \n",
    "*\"Who is the current president of the United States, and what is their age?\"*\n",
    "\n",
    "**Prompt Structure:**  \n",
    "You instruct the AI to alternate between Thought (reason), Action (do), and Observation (see result).\n",
    "\n",
    "#### **Step-by-Step Example**\n",
    "\n",
    "**User Prompt:**  \n",
    "```\n",
    "Answer the question step by step. For each step, alternate between:\n",
    "- Thought: What should you do next?\n",
    "- Action: Take an action (e.g., search, calculate).\n",
    "- Observation: What did you find?\n",
    "Repeat until you can answer the question.\n",
    "Question: Who is the current president of the United States, and what is their age?\n",
    "```\n",
    "\n",
    "**Model Response:**\n",
    "\n",
    "1. **Thought:** I need to find out who the current president of the United States is.\n",
    "2. **Action:** Search[\"current president of the United States\"]\n",
    "3. **Observation:** The search result says \"Joe Biden is the current president.\"\n",
    "4. **Thought:** Now I need to find out Joe Biden's age.\n",
    "5. **Action:** Search[\"Joe Biden age\"]\n",
    "6. **Observation:** The search result says \"Joe Biden is 82 years old.\"\n",
    "7. **Final Answer:** The current president of the United States is Joe Biden, who is 82 years old.\n",
    "\n",
    "### **How Is This Used Further?**\n",
    "\n",
    "- **In AI agents:** Each action could be a real API call, database query, or tool use.\n",
    "- **In customer support:** The AI could reason about a complaint, look up order status, and recommend solutions.\n",
    "- **In research:** The AI could plan a series of searches, summarize findings, and synthesize an answer.\n",
    "\n",
    "## **Summary Table**\n",
    "\n",
    "| Step       | What Happens                                    | Example in Practice                                 |\n",
    "|------------|-------------------------------------------------|-----------------------------------------------------|\n",
    "| Thought    | AI decides what to do next                      | \"I need to find the president's name.\"              |\n",
    "| Action     | AI performs an action (search, tool, etc.)      | Search[\"current president of the US\"]               |\n",
    "| Observation| AI sees the result of the action                | \"Result: Joe Biden is the president.\"               |\n",
    "| Repeat     | AI reasons again, takes next action if needed   | \"Now, find Joe Biden's age.\"                        |\n",
    "| Final Answer| AI gives the complete answer                   | \"Joe Biden, age 82.\"                                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c940169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 4. ReAct Loop Implementation ---\n",
    "# def react_loop(user_question):\n",
    "#     history = []  # To store the AI's reasoning and actions\n",
    "#     observation = \"\"\n",
    "#     max_steps = 5  # Prevent infinite loops\n",
    "#     for step in range(max_steps):\n",
    "#         # 1. Build prompt with history, user question, and last observation\n",
    "#         prompt = (\n",
    "#             \"You are an assistant that can reason and use tools.\\n\"\n",
    "#             \"When you need to use a tool, write: Action: get_weather(city)\\n\"\n",
    "#             \"I will execute the action and return the result as Observation.\\n\"\n",
    "#             \"Repeat Thought/Action/Observation until you can answer.\\n\"\n",
    "#             f\"User question: {user_question}\\n\"\n",
    "#         )\n",
    "#         for entry in history:\n",
    "#             prompt += entry + \"\\n\"\n",
    "#         if observation:\n",
    "#             prompt += f\"Observation: {observation}\\n\"\n",
    "\n",
    "#         prompt += \"Thought:\"\n",
    "\n",
    "#         # # 2. Get AI's next step\n",
    "#         # response = llm(prompt, do_sample=False)[0]['generated_text']\n",
    "#         # # Extract only the new completion after the last prompt\n",
    "#         # next_line = response[len(prompt):].strip().split(\"\\n\")[0]\n",
    "#         response = llm(prompt)\n",
    "#         # Extract only the new completion after the last prompt\n",
    "#         next_line = response.strip().split(\"\\n\")[0]\n",
    "\n",
    "#         print(f\"AI: {next_line}\")\n",
    "\n",
    "#         # 3. Check if AI wants to act\n",
    "#         if \"Action:\" in next_line:\n",
    "#             # Parse city from action\n",
    "#             import re\n",
    "#             match = re.search(r'get_weather\\((.*?)\\)', next_line)\n",
    "#             if match:\n",
    "#                 city = match.group(1).strip().strip('\"').strip(\"'\")\n",
    "#                 observation = get_weather(city)\n",
    "#                 history.append(f\"Thought:{next_line}\")\n",
    "#                 history.append(f\"Action: get_weather({city})\")\n",
    "#                 history.append(f\"Observation: {observation}\")\n",
    "#             else:\n",
    "#                 observation = \"Invalid action format.\"\n",
    "#                 history.append(f\"Thought:{next_line}\")\n",
    "#                 history.append(\"Action: none\")\n",
    "#                 history.append(f\"Observation: {observation}\")\n",
    "#         elif \"Final Answer:\" in next_line or \"Answer:\" in next_line:\n",
    "#             print(next_line)\n",
    "#             break\n",
    "#         else:\n",
    "#             # If just reasoning, add to history\n",
    "#             history.append(f\"Thought:{next_line}\")\n",
    "\n",
    "def react_loop(user_question):\n",
    "    history = []\n",
    "    observation = \"\"\n",
    "    max_steps = 5\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Build the prompt as a chat history\n",
    "        prompt = (\n",
    "            \"You are an AI assistant that can reason step by step and use tools.\\n\"\n",
    "            \"When you need to use a tool, write: Action: get_weather(city)\\n\"\n",
    "            \"I will execute the action and return the result as Observation.\\n\"\n",
    "            \"Repeat Thought/Action/Observation until you can answer.\\n\"\n",
    "            f\"User question: {user_question}\\n\"\n",
    "        )\n",
    "        for entry in history:\n",
    "            prompt += entry + \"\\n\"\n",
    "        if observation:\n",
    "            prompt += f\"Observation: {observation}\\n\"\n",
    "        prompt += \"Thought:\"\n",
    "\n",
    "        # Print the full prompt (including Observations) for debugging\n",
    "        print(\"\\n--- Prompt sent to Gemini ---\")\n",
    "        print(prompt)\n",
    "        print(\"--- End of Prompt ---\\n\")\n",
    "\n",
    "        # Gemini expects a list of messages (chat format)\n",
    "        messages = [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}]\n",
    "        response = gemini_llm(messages)\n",
    "        print(f\"AI: {response}\")\n",
    "\n",
    "        # Parse for Action\n",
    "        if \"Action:\" in response:\n",
    "            import re\n",
    "            import time # Import time for delay between requests\n",
    "            matches = re.findall(r'get_weather\\((.*?)\\)', response)\n",
    "            for city in matches:\n",
    "                time.sleep(2)  # Wait 2 seconds between requests to avoid rate limiting\n",
    "                # Clean up the city name\n",
    "                city = city.strip().strip('\"').strip(\"'\")\n",
    "                #print(f\"Fetching weather for {city}...\")  # Debug\n",
    "                observation = get_weather(city)\n",
    "                print(f\"The weather received from Weather API function is Observation: {observation}\")\n",
    "                history.append(f\"Action: get_weather({city})\")\n",
    "                history.append(f\"Observation: {observation}\")\n",
    "                print(f\"Updated history: {history}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "492dd5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prompt sent to Gemini ---\n",
      "You are an AI assistant that can reason step by step and use tools.\n",
      "When you need to use a tool, write: Action: get_weather(city)\n",
      "I will execute the action and return the result as Observation.\n",
      "Repeat Thought/Action/Observation until you can answer.\n",
      "User question: What is weather of Bangalore and Pune ?\n",
      "Thought:\n",
      "--- End of Prompt ---\n",
      "\n",
      "AI: Thought: I need to get the weather for Bangalore and Pune separately.  I'll use the `get_weather` tool for each city.\n",
      "\n",
      "Action: get_weather(Bangalore)\n",
      "\n",
      "Fetching weather for Bangalore...\n",
      "The weather received from Weather API function is Observation: The current temperature in Bangalore is 22.9°C.\n",
      "Updated history: ['Action: get_weather(Bangalore)', 'Observation: The current temperature in Bangalore is 22.9°C.']\n",
      "\n",
      "--- Prompt sent to Gemini ---\n",
      "You are an AI assistant that can reason step by step and use tools.\n",
      "When you need to use a tool, write: Action: get_weather(city)\n",
      "I will execute the action and return the result as Observation.\n",
      "Repeat Thought/Action/Observation until you can answer.\n",
      "User question: What is weather of Bangalore and Pune ?\n",
      "Action: get_weather(Bangalore)\n",
      "Observation: The current temperature in Bangalore is 22.9°C.\n",
      "Observation: The current temperature in Bangalore is 22.9°C.\n",
      "Thought:\n",
      "--- End of Prompt ---\n",
      "\n",
      "AI: Thought: I have the weather for Bangalore. I need to get the weather for Pune.\n",
      "\n",
      "Action: get_weather(Pune)\n",
      "\n",
      "Fetching weather for Pune...\n",
      "The weather received from Weather API function is Observation: The current temperature in Pune is 23.9°C.\n",
      "Updated history: ['Action: get_weather(Bangalore)', 'Observation: The current temperature in Bangalore is 22.9°C.', 'Action: get_weather(Pune)', 'Observation: The current temperature in Pune is 23.9°C.']\n",
      "\n",
      "--- Prompt sent to Gemini ---\n",
      "You are an AI assistant that can reason step by step and use tools.\n",
      "When you need to use a tool, write: Action: get_weather(city)\n",
      "I will execute the action and return the result as Observation.\n",
      "Repeat Thought/Action/Observation until you can answer.\n",
      "User question: What is weather of Bangalore and Pune ?\n",
      "Action: get_weather(Bangalore)\n",
      "Observation: The current temperature in Bangalore is 22.9°C.\n",
      "Action: get_weather(Pune)\n",
      "Observation: The current temperature in Pune is 23.9°C.\n",
      "Observation: The current temperature in Pune is 23.9°C.\n",
      "Thought:\n",
      "--- End of Prompt ---\n",
      "\n",
      "AI: Thought: I have the current temperature for both Bangalore and Pune.  I can now answer the user's question.  Bangalore's temperature is 22.9°C and Pune's temperature is 23.9°C.\n",
      "Answer: The current temperature in Bangalore is 22.9°C and in Pune is 23.9°C.\n",
      "\n",
      "\n",
      "--- Prompt sent to Gemini ---\n",
      "You are an AI assistant that can reason step by step and use tools.\n",
      "When you need to use a tool, write: Action: get_weather(city)\n",
      "I will execute the action and return the result as Observation.\n",
      "Repeat Thought/Action/Observation until you can answer.\n",
      "User question: What is weather of Bangalore and Pune ?\n",
      "Action: get_weather(Bangalore)\n",
      "Observation: The current temperature in Bangalore is 22.9°C.\n",
      "Action: get_weather(Pune)\n",
      "Observation: The current temperature in Pune is 23.9°C.\n",
      "Observation: The current temperature in Pune is 23.9°C.\n",
      "Thought:\n",
      "--- End of Prompt ---\n",
      "\n",
      "AI: Thought: I have the current temperature for both Bangalore and Pune.  I can now answer the user's question.  Bangalore's temperature is 22.9°C and Pune's temperature is 23.9°C.\n",
      "Answer: The current temperature in Bangalore is 22.9°C and in Pune is 23.9°C.\n",
      "\n",
      "\n",
      "--- Prompt sent to Gemini ---\n",
      "You are an AI assistant that can reason step by step and use tools.\n",
      "When you need to use a tool, write: Action: get_weather(city)\n",
      "I will execute the action and return the result as Observation.\n",
      "Repeat Thought/Action/Observation until you can answer.\n",
      "User question: What is weather of Bangalore and Pune ?\n",
      "Action: get_weather(Bangalore)\n",
      "Observation: The current temperature in Bangalore is 22.9°C.\n",
      "Action: get_weather(Pune)\n",
      "Observation: The current temperature in Pune is 23.9°C.\n",
      "Observation: The current temperature in Pune is 23.9°C.\n",
      "Thought:\n",
      "--- End of Prompt ---\n",
      "\n",
      "AI: Thought: I have the current temperature for both Bangalore and Pune.  I can now answer the user's question.  Bangalore's temperature is 22.9°C and Pune's temperature is 23.9°C.\n",
      "Answer: The current temperature in Bangalore is 22.9°C and in Pune is 23.9°C.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_q = input(\"Ask a weather-related question: \")\n",
    "    react_loop(user_q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
