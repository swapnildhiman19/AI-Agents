{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "132f070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/codespace/.local/lib/python3.12/site-packages (2.7.0+cpu)\n",
      "Requirement already satisfied: transformers in /home/codespace/.python/current/lib/python3.12/site-packages (4.53.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/codespace/.python/current/lib/python3.12/site-packages (0.33.2)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers huggingface_hub requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f682af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /home/codespace/.python/current/lib/python3.12/site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978ed3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch HF_TOKEN\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c3d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4b5dc",
   "metadata": {},
   "source": [
    "Authenticate with Hugging Face (Gemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b556b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff83197",
   "metadata": {},
   "source": [
    "# Loading a Hugging Face Model : Gemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f135022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a9c62e4e4542368f7e6b7bca64d805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model_name = \"google/gemma-2b-it\" #This is a big model and codespace is not able to run it, replacing it with a smaller one\n",
    "model_name = \"microsoft/phi-2\"  # A smaller model for demonstration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "# Downloads and loads the tokenizer for DistilGPT2. \n",
    "# The tokenizer converts raw text into tokens (numbers) that the model can understand. \n",
    "# It uses the same byte-level BPE as GPT-2. This is where the text is converted to embeddings (indirectly): the tokenizer outputs token IDs,\n",
    "# which the model then turns into embeddings in its first layer.\n",
    "\n",
    "# Downloads and loads the pre-trained DistilGPT2 model. This is the neural network that generates text, given the input tokens.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=hf_token)\n",
    "\n",
    "#The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.\n",
    "# Pipeline we get from Hugging Face, text-generation parameter should be compatible with the model we are using.\n",
    "#Wraps the model and tokenizer in a high-level pipeline for easy text generation. Now you can just call llm(\"Your prompt\") to generate text. max_new_tokens=256 limits the output length.\n",
    "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b267d8",
   "metadata": {},
   "source": [
    "# Weather tooling function : Our function to get the weather. Ideally our LLM Model should be able to call this function according to the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb9f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Weather Tool Function (using Open-Meteo, free, no auth needed) ---\n",
    "\n",
    "def get_weather(city):\n",
    "    # Geocode city to lat/lon (using Open-Meteo's geocoding)\n",
    "    geo = requests.get(f\"https://geocoding-api.open-meteo.com/v1/search?name={city}\").json()\n",
    "    '''\n",
    "    - Defines a function `get_weather` that takes a city name as input.\n",
    "    - Sends a GET request to the Open-Meteo geocoding API to find the latitude and longitude of the city. The response is converted to JSON.\n",
    "    '''\n",
    "\n",
    "    if not geo.get(\"results\"):\n",
    "        return f\"Could not find location for {city}.\"\n",
    "    '''- Checks if the API response contains a `\"results\"` key. If not, it returns a message saying the location couldn't be found.'''\n",
    "\n",
    "    lat, lon = geo[\"results\"][0][\"latitude\"], geo[\"results\"][0][\"longitude\"]\n",
    "\n",
    "    # Get weather, Sends another GET request to the Open-Meteo weather API, using the latitude and longitude found earlier, to get the current weather. The response is converted to JSON.\n",
    "    weather = requests.get(\n",
    "        f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true\"\n",
    "    ).json()\n",
    "    if \"current_weather\" in weather:\n",
    "        temp = weather[\"current_weather\"][\"temperature\"]\n",
    "        desc = f\"The current temperature in {city} is {temp}°C.\"\n",
    "        return desc\n",
    "    else:\n",
    "        return \"Weather data not available.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ee5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather Tool Ready. Example usage: The current temperature in Bangalore is 27.5°C.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weather Tool Ready. Example usage: {get_weather('Bangalore')}\") # Testing the weather function: Weather Tool Ready. Example usage: The current temperature in Bangalore is 27.0°C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c61a4",
   "metadata": {},
   "source": [
    "# React Prompting : Reasong and Acting :\n",
    "\n",
    "Here’s a clear explanation of **ReAct prompting**—first in layman’s terms, then in technical detail, with a practical example showing how prompts and model responses work at each step.\n",
    "\n",
    "## **Layman’s Explanation**\n",
    "\n",
    "**ReAct prompting** stands for **Reasoning and Acting**.  \n",
    "It’s a way to get AI to not just “think” (reason) about a problem, but also “do” (act) something step by step—just like a person solving a puzzle:\n",
    "\n",
    "- **Reason:** The AI explains what it’s thinking or planning.\n",
    "- **Act:** The AI takes an action (like looking something up, using a tool, or searching a database).\n",
    "- **Observe:** The AI sees what happened after the action.\n",
    "- **Repeat:** The AI reasons again, takes another action if needed, and continues until it finds the answer.\n",
    "\n",
    "**Why use it?**  \n",
    "It helps the AI solve complex problems more accurately, transparently, and flexibly—especially when it needs to interact with tools or external information[1][2][3][4][5].\n",
    "\n",
    "## **Technical Deep Dive**\n",
    "\n",
    "### **What is ReAct Prompting?**\n",
    "\n",
    "- **ReAct** is a prompting technique that combines:\n",
    "  - **Reasoning:** The AI breaks down the problem, thinks through possible solutions, and plans what to do next.\n",
    "  - **Acting:** The AI takes concrete steps (actions) based on its reasoning, such as searching, calculating, or calling a function.\n",
    "- This process is **iterative**: The AI alternates between reasoning and acting, using the outcome of each action to inform its next step[1][2][3][4][5].\n",
    "\n",
    "### **Key Benefits**\n",
    "\n",
    "- **Transparency:** You can see the AI’s thought process.\n",
    "- **Accuracy:** The AI checks its own work at each step.\n",
    "- **Adaptability:** The AI can adjust its approach if new information appears.\n",
    "- **Reduces Hallucination:** By grounding actions in real observations, the AI is less likely to make things up[1][4][5].\n",
    "\n",
    "### **How ReAct Prompting Works (Step-by-Step Example)**\n",
    "\n",
    "**Scenario:**  \n",
    "Suppose you want the AI to answer:  \n",
    "*\"Who is the current president of the United States, and what is their age?\"*\n",
    "\n",
    "**Prompt Structure:**  \n",
    "You instruct the AI to alternate between Thought (reason), Action (do), and Observation (see result).\n",
    "\n",
    "#### **Step-by-Step Example**\n",
    "\n",
    "**User Prompt:**  \n",
    "```\n",
    "Answer the question step by step. For each step, alternate between:\n",
    "- Thought: What should you do next?\n",
    "- Action: Take an action (e.g., search, calculate).\n",
    "- Observation: What did you find?\n",
    "Repeat until you can answer the question.\n",
    "Question: Who is the current president of the United States, and what is their age?\n",
    "```\n",
    "\n",
    "**Model Response:**\n",
    "\n",
    "1. **Thought:** I need to find out who the current president of the United States is.\n",
    "2. **Action:** Search[\"current president of the United States\"]\n",
    "3. **Observation:** The search result says \"Joe Biden is the current president.\"\n",
    "4. **Thought:** Now I need to find out Joe Biden's age.\n",
    "5. **Action:** Search[\"Joe Biden age\"]\n",
    "6. **Observation:** The search result says \"Joe Biden is 82 years old.\"\n",
    "7. **Final Answer:** The current president of the United States is Joe Biden, who is 82 years old.\n",
    "\n",
    "### **How Is This Used Further?**\n",
    "\n",
    "- **In AI agents:** Each action could be a real API call, database query, or tool use.\n",
    "- **In customer support:** The AI could reason about a complaint, look up order status, and recommend solutions.\n",
    "- **In research:** The AI could plan a series of searches, summarize findings, and synthesize an answer.\n",
    "\n",
    "## **Summary Table**\n",
    "\n",
    "| Step       | What Happens                                    | Example in Practice                                 |\n",
    "|------------|-------------------------------------------------|-----------------------------------------------------|\n",
    "| Thought    | AI decides what to do next                      | \"I need to find the president's name.\"              |\n",
    "| Action     | AI performs an action (search, tool, etc.)      | Search[\"current president of the US\"]               |\n",
    "| Observation| AI sees the result of the action                | \"Result: Joe Biden is the president.\"               |\n",
    "| Repeat     | AI reasons again, takes next action if needed   | \"Now, find Joe Biden's age.\"                        |\n",
    "| Final Answer| AI gives the complete answer                   | \"Joe Biden, age 82.\"                                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. ReAct Loop Implementation ---\n",
    "def react_loop(user_question):\n",
    "    history = []  # To store the AI's reasoning and actions\n",
    "    observation = \"\"\n",
    "    max_steps = 5  # Prevent infinite loops\n",
    "    for step in range(max_steps):\n",
    "        # 1. Build prompt with history, user question, and last observation\n",
    "        prompt = (\n",
    "            \"You are an assistant that can reason and use tools.\\n\"\n",
    "            \"When you need to use a tool, write: Action: get_weather(city)\\n\"\n",
    "            \"I will execute the action and return the result as Observation.\\n\"\n",
    "            \"Repeat Thought/Action/Observation until you can answer.\\n\"\n",
    "            f\"User question: {user_question}\\n\"\n",
    "        )\n",
    "        for entry in history:\n",
    "            prompt += entry + \"\\n\"\n",
    "        if observation:\n",
    "            prompt += f\"Observation: {observation}\\n\"\n",
    "\n",
    "        prompt += \"Thought:\"\n",
    "\n",
    "        # 2. Get AI's next step\n",
    "        response = llm(prompt, do_sample=False)[0]['generated_text']\n",
    "        # Extract only the new completion after the last prompt\n",
    "        next_line = response[len(prompt):].strip().split(\"\\n\")[0]\n",
    "\n",
    "        print(f\"AI: {next_line}\")\n",
    "\n",
    "        # 3. Check if AI wants to act\n",
    "        if \"Action:\" in next_line:\n",
    "            # Parse city from action\n",
    "            import re\n",
    "            match = re.search(r'get_weather\\((.*?)\\)', next_line)\n",
    "            if match:\n",
    "                city = match.group(1).strip().strip('\"').strip(\"'\")\n",
    "                observation = get_weather(city)\n",
    "                history.append(f\"Thought:{next_line}\")\n",
    "                history.append(f\"Action: get_weather({city})\")\n",
    "                history.append(f\"Observation: {observation}\")\n",
    "            else:\n",
    "                observation = \"Invalid action format.\"\n",
    "                history.append(f\"Thought:{next_line}\")\n",
    "                history.append(\"Action: none\")\n",
    "                history.append(f\"Observation: {observation}\")\n",
    "        elif \"Final Answer:\" in next_line or \"Answer:\" in next_line:\n",
    "            print(next_line)\n",
    "            break\n",
    "        else:\n",
    "            # If just reasoning, add to history\n",
    "            history.append(f\"Thought:{next_line}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492dd5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: What is the weather in Bangalore?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: What is the weather in Bangalore?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: What is the weather in Bangalore?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: What is the weather in Bangalore?\n",
      "AI: What is the weather in Bangalore?\n"
     ]
    }
   ],
   "source": [
    "user_q = \"What is the weather in Bangalore?\"  # <-- Change this to any city/question you want\n",
    "react_loop(user_q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
